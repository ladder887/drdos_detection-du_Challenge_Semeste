# -*- coding: utf-8 -*-
"""Train_2023DRDoS_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V8yh4Y6Zyo5XlPK9ypS36wFtqdSgTkZI

# LOAD DATA
"""

# Commented out IPython magic to ensure Python compatibility.
# import relevant modules
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
#import seaborn as sns
import sklearn
#import imblearn
from sklearn.neighbors import LocalOutlierFactor
import tensorflow as tf

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

dataset = pd.read_csv('192_168_0_15_total.csv')

dataset.head()

dataset = dataset.drop(['timestamp'], axis=1)

print(dataset['Label'].value_counts())

#======= 중요!! 선택!! ======~#
# Binary 분류일 때 아래 코드 주석제거!# Multiclass 분류일 때는 코드 없음!
dataset['Label'] = dataset['Label'].replace(['Attack_1', 'Attack_2', 'Attack_3'], 'Attack ')



print(dataset['Label'].value_counts())

dataset = dataset.dropna()
len(dataset)
#dataset.columns

nans = lambda df: df[df.isnull().any(axis=1)]
nans(dataset)

#  Label 컬럼 안에 담긴 고유의 카테고리 값을 확인
dataset['Label'].unique()
print(dataset['Label'].value_counts())

"""# EXPLORATORY ANALYSIS"""

# Label을 제외한 컬럼선택
index = dataset.drop(['Label'], axis=1)
index.head()

#object인 부분 데이터 타입 바꾸는 법
index = index.apply(lambda col:pd.to_numeric(col, errors='coerce'))

#print(index.info())

# Label을 제외한 컬럼선택
label = dataset[['Label']]

frames = [index, label]
dataset2 = pd.concat(frames, axis=1)

from sklearn.model_selection import train_test_split

# X와 Y를 훈련 데이터(train)와 테스트 데이터(test)로 분류 = 70 : 30
train, test = train_test_split(dataset2, test_size = 0.3, random_state = 0)

print(train['Label'].unique())
print(test['Label'].unique())
print(train['Label'].value_counts())
print(test['Label'].value_counts())

## 일부만 테스트
#train = train.iloc[:20000, :]
#test = test.iloc[:10000, :]

"""# SCALING NUMERICAL ATTRIBUTES"""

#from sklearn.preprocessing import StandardScaler
#scaler = StandardScaler() # 가장 정확성이 높은 범위설정가능, ex) (0,1) 또는 (0, 10) 또는 (0,100) 또는 (0,1000) 등

from sklearn.preprocessing import MinMaxScaler # 각 열의 최소값, 최대값을 기준으로 리스케일 ex) 최소값을 0, 최대값을 1로 하여 수치조정
scaler = MinMaxScaler(feature_range=(0, 10)) # 가장 정확성이 높은 범위설정가능, ex) (0,1) 또는 (0, 10) 또는 (0,100) 또는 (0,1000) 등

# extract numerical attributes and scale it to have zero mean and unit variance
cols = train.select_dtypes(include=['float64','int64']).columns

sc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))
sc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))

# turn the result back to a dataframe
sc_traindf = pd.DataFrame(sc_train, columns = cols)
sc_testdf = pd.DataFrame(sc_test, columns = cols)

"""# ENCODING CATEGORICAL ATTRIBUTES"""

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

## extract categorical attributes from both training and test sets
cattrain = train.select_dtypes(include=['object']).copy()
cattest = test.select_dtypes(include=['object']).copy()

## check the encoded mapping
encoder.fit(cattrain['Label'])
encoder_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
print(encoder_name_mapping)

encoder.fit(cattest['Label'])
encoder_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
print(encoder_name_mapping)

## encode the categorical attributes
traincat = cattrain.apply(encoder.fit_transform)
testcat = cattest.apply(encoder.fit_transform)

## separate target column from encoded data
enctrain = traincat.drop(['Label'], axis=1) #만약 Label를 제외한 다른 텍스트값을 갖는 속성이 있었다면, 필요한 작업임
cat_Ytrain = traincat[['Label']].copy() # 확인값으로 사용하기 위함

enctest = testcat.drop(['Label'], axis=1)
cat_Ytest = testcat[['Label']].copy()

enctrain.index = range(len(enctrain))
cat_Ytrain.index = range(len(cat_Ytrain))

enctest.index = range(len(enctest))
cat_Ytest.index = range(len(cat_Ytest))

trainX = pd.concat([sc_traindf,enctrain],axis=1)
trainY = cat_Ytrain

testX = pd.concat([sc_testdf,enctest],axis=1)
testY = cat_Ytest

"""# RNN model"""

# 특정 열만 살펴보고 싶은 경우
#index = 2 #원하는 지표의 index 값으로 설정
#trainX = trainX.iloc[:, index]
#testX = testX.iloc[:, index]

# 지표를 범위로 선택하는 경우 -> 현재는 전체 지표 모두 반영
trainX = trainX.iloc[:, :]
testX = testX.iloc[:, :]
trainY = trainY.iloc[:, :]
testY= testY.iloc[:, :]

print(trainX.isnull().values.any())
print(testX.isnull().values.any())
print(trainY.isnull().values.any())
print(testY.isnull().values.any())
cntClass = len(trainY['Label'].unique())
cntFeatures = len(trainX.columns)

print(trainY['Label'].unique())
print(testY['Label'].unique())

len(trainY['Label'].unique())

from keras.layers import SimpleRNN, Embedding, Dense, LSTM
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

model = Sequential()

# 임베딩 벡터의 차원 설정
dim = 15 # 다른 값을 넣어 실험 가능 -> 5 -> 10-> 15 등등
model.add(Embedding(len(trainX), dim))
model.add(LSTM(dim)) # RNN 셀의 hidden_size는 dim   # SimpleRNN or LSTM
model.add(Dense(cntClass, activation='softmax')) #분류에 따라 다르게

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])

trainY = to_categorical(trainY, num_classes=cntClass)
#print(trainX)
#print(trainY)
print(len(trainX))
print(len(trainY))
history = model.fit(trainX, trainY, epochs=20, batch_size=100, validation_split=0.2)

epochs = range(1, len(history.history['accuracy']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('train_loss vs val_loss')
plt.ylabel('loss')
plt.xlabel('number of Epochs')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

epochs = range(1, len(history.history['accuracy']) + 1)
plt.plot(epochs, history.history['accuracy'])
plt.plot(epochs, history.history['val_accuracy'])
plt.title('train_acc vs val_acc')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='lower right')
plt.show()

testY = to_categorical(testY, num_classes=cntClass)

loss, accuracy = model.evaluate(testX, testY, verbose=0)
print("loss: %.4f" % (loss))
print("accuracy: %.4f" % (accuracy))

from sklearn.metrics import classification_report, confusion_matrix

Y_pred = model.predict(testX)
#print(Y_pred)
y_pred = np.argmax(Y_pred, axis=1)
#print(y_pred)

y_pred = model.predict(testX)

predict_classes=np.argmax(y_pred,axis=1)

target_names = list(map(str,encoder.transform(encoder.classes_)))
#print(testX)
print(classification_report(np.argmax(testY, axis=1), predict_classes, target_names = target_names))
print(confusion_matrix(np.argmax(testY, axis=1), predict_classes))

#f1-score를 각 클래스의 정확도로 보면됨

# 선택!!

model.save("RNN_binary_model.h5")
#model.save("RNN_multiclass_model.h5")